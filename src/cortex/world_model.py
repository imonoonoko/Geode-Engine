# world_model.py
# Phase 14: ä¸–ç•Œãƒ¢ãƒ‡ãƒ« (World Model)
# ã€Œè¡Œå‹•ã—ãŸã‚‰ã©ã†ãªã‚‹ã‹ã‚’å†…éƒ¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€

import time
import threading
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from collections import deque


@dataclass
class StatePrediction:
    """çŠ¶æ…‹äºˆæ¸¬ã®è¨˜éŒ²"""
    state_before: Dict[str, float]
    action: str
    predicted_state: Dict[str, float]
    actual_state: Optional[Dict[str, float]] = None
    error: float = 0.0
    timestamp: float = field(default_factory=time.time)


class WorldModel:
    """
    ä¸–ç•Œãƒ¢ãƒ‡ãƒ«: è¡Œå‹•ã®çµæœã‚’äºˆæ¸¬
    
    p(s_{t+1} | s_t, a_t) ã‚’è¿‘ä¼¼ã—ã€
    äºˆæ¸¬èª¤å·®ã‚’å­¦ç¿’ä¿¡å·ã¨ã—ã¦æ´»ç”¨ã€‚
    
    Active Inference ã®æ ¸å¿ƒéƒ¨åˆ†ã€‚
    """
    
    def __init__(self, brain=None):
        self.brain = brain
        self.lock = threading.Lock()
        
        # çŠ¶æ…‹é·ç§»ãƒ¢ãƒ‡ãƒ«: (action, state_key) -> expected_delta
        self.transition_model: Dict[Tuple[str, str], float] = {}
        
        # äºˆæ¸¬å±¥æ­´
        self.prediction_history: deque = deque(maxlen=100)
        
        # äºˆæ¸¬èª¤å·®ã®å±¥æ­´
        self.error_history: deque = deque(maxlen=100)
        
        # å­¦ç¿’ç‡
        self.learning_rate = 0.1
        
        print("ğŸŒ World Model Initialized.")
    
    def predict(self, state: Dict[str, float], action: str) -> Dict[str, float]:
        """
        æ¬¡ã®çŠ¶æ…‹ã‚’äºˆæ¸¬: p(s_{t+1} | s_t, a_t)
        
        Args:
            state: ç¾åœ¨ã®çŠ¶æ…‹ï¼ˆãƒ›ãƒ«ãƒ¢ãƒ³ãƒ¬ãƒ™ãƒ«ç­‰ï¼‰
            action: äºˆå®šã—ã¦ã„ã‚‹è¡Œå‹•
            
        Returns:
            äºˆæ¸¬ã•ã‚Œã‚‹æ¬¡ã®çŠ¶æ…‹
        """
        predicted = {}
        
        with self.lock:
            for key, value in state.items():
                # é·ç§»ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰äºˆæ¸¬å¤‰åŒ–é‡ã‚’å–å¾—
                delta = self.transition_model.get((action, key), 0.0)
                predicted[key] = value + delta
        
        # äºˆæ¸¬ã‚’è¨˜éŒ²
        prediction = StatePrediction(
            state_before=state.copy(),
            action=action,
            predicted_state=predicted
        )
        
        with self.lock:
            self.prediction_history.append(prediction)
        
        return predicted
    
    def update(self, predicted: Dict[str, float], actual: Dict[str, float], action: str) -> float:
        """
        äºˆæ¸¬èª¤å·®ã‹ã‚‰å­¦ç¿’
        
        Args:
            predicted: äºˆæ¸¬ã—ãŸçŠ¶æ…‹
            actual: å®Ÿéš›ã®çŠ¶æ…‹
            action: è¡Œã£ãŸè¡Œå‹•
            
        Returns:
            äºˆæ¸¬èª¤å·®
        """
        total_error = 0.0
        
        with self.lock:
            for key in predicted:
                if key not in actual:
                    continue
                
                error = actual[key] - predicted[key]
                total_error += abs(error)
                
                # é·ç§»ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°
                current_delta = self.transition_model.get((action, key), 0.0)
                new_delta = current_delta + self.learning_rate * error
                self.transition_model[(action, key)] = new_delta
            
            # èª¤å·®ã‚’è¨˜éŒ²
            self.error_history.append(total_error)
            
            # æœ€å¾Œã®äºˆæ¸¬ã‚’æ›´æ–°
            if self.prediction_history:
                last = self.prediction_history[-1]
                last.actual_state = actual.copy()
                last.error = total_error
        
        return total_error
    
    def get_prediction_error(self) -> float:
        """
        ç›´è¿‘ã®äºˆæ¸¬èª¤å·®ã‚’å–å¾—
        
        Returns:
            å¹³å‡äºˆæ¸¬èª¤å·®
        """
        with self.lock:
            if not self.error_history:
                return 0.0
            
            recent = list(self.error_history)[-10:]
            return sum(recent) / len(recent)
    
    def simulate(self, state: Dict[str, float], actions: List[str]) -> List[Dict[str, float]]:
        """
        è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®å†…éƒ¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        
        Args:
            state: åˆæœŸçŠ¶æ…‹
            actions: è¡Œå‹•åˆ—
            
        Returns:
            äºˆæ¸¬ã•ã‚Œã‚‹çŠ¶æ…‹åˆ—
        """
        trajectory = [state.copy()]
        current = state.copy()
        
        for action in actions:
            predicted = self.predict(current, action)
            trajectory.append(predicted)
            current = predicted
        
        return trajectory
    
    def get_best_action(self, state: Dict[str, float], candidates: List[str], 
                        goal_key: str, maximize: bool = True) -> str:
        """
        ç›®æ¨™ã«æœ€é©ãªè¡Œå‹•ã‚’é¸æŠ
        
        Args:
            state: ç¾åœ¨ã®çŠ¶æ…‹
            candidates: è¡Œå‹•å€™è£œ
            goal_key: æœ€é©åŒ–å¯¾è±¡ã®çŠ¶æ…‹ã‚­ãƒ¼
            maximize: True=æœ€å¤§åŒ–, False=æœ€å°åŒ–
            
        Returns:
            æœ€é©ãªè¡Œå‹•
        """
        best_action = candidates[0] if candidates else ""
        best_value = float('-inf') if maximize else float('inf')
        
        for action in candidates:
            predicted = self.predict(state, action)
            value = predicted.get(goal_key, 0.0)
            
            if maximize and value > best_value:
                best_value = value
                best_action = action
            elif not maximize and value < best_value:
                best_value = value
                best_action = action
        
        return best_action
    
    def get_state(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®çŠ¶æ…‹ã‚’å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        with self.lock:
            return {
                "transition_model_size": len(self.transition_model),
                "prediction_count": len(self.prediction_history),
                "avg_error": self.get_prediction_error()
            }
