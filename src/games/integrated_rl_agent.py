# integrated_rl_agent.py
# Game AI: Kaname çµ±åˆå‹å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
# GeologicalMemory, WorldModel, MetaLearner ã¨é€£æº

import time
import random
import numpy as np
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
import threading


@dataclass
class GameExperience:
    """ã‚²ãƒ¼ãƒ çµŒé¨“ï¼ˆåœ°è³ªå­¦çš„å †ç©ç”¨ï¼‰"""
    game_type: str
    state_hash: str
    action: int
    reward: float
    outcome: str  # "success", "failure", "neutral"
    emotion: float  # æ„Ÿæƒ…ä¾¡ (-1 to 1)
    timestamp: float = field(default_factory=time.time)


class IntegratedRLAgent:
    """
    Kaname çµ±åˆå‹å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    
    - GeologicalMemory: ã‚²ãƒ¼ãƒ çµŒé¨“ã‚’åœ°è³ªå­¦çš„ã«å †ç©
    - WorldModel: ã‚²ãƒ¼ãƒ çŠ¶æ…‹ã‚’äºˆæ¸¬
    - MetaLearner: å­¦ç¿’ç‡ã‚’å‹•çš„èª¿æ•´
    """
    
    def __init__(self, 
                 action_size: int,
                 brain=None,
                 epsilon: float = 1.0,  # æ¢ç´¢ç‡ï¼ˆCuriosityï¼‰ã¨ã—ã¦ä½¿ç”¨
                 epsilon_min: float = 0.1,
                 epsilon_decay: float = 0.995,
                 gamma: float = 0.99):
        """
        Args:
            action_size: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°
            brain: Kaname ã® Brain ã¸ã®å‚ç…§
            epsilon: æ¢ç´¢ç‡ï¼ˆCuriosityï¼‰
            epsilon_min: æœ€å°æ¢ç´¢ç‡
            epsilon_decay: æ¸›è¡°
            gamma: å‰²å¼•ç‡
        """
        self.action_size = action_size
        self.brain = brain
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.gamma = gamma
        
        # Q-table ã¯å»ƒæ­¢ï¼ˆå¼·åŒ–å­¦ç¿’ã‚’ä½¿ã‚ãšã€èƒ½å‹•çš„æ¨è«–ã‚’ä½¿ç”¨ï¼‰
        # self.q_table = {} 
        self.lock = threading.Lock()
        
        # çµ±è¨ˆ
        self.total_steps = 0
        self.training_steps = 0
        self.episode_count = 0
        self.prediction_errors: List[float] = []
        
        # Kaname ã‚·ã‚¹ãƒ†ãƒ ã¸ã®å‚ç…§
        self.meta_learner = None
        self.world_model = None
        self.memory = None
        
        self._init_kaname_systems()
        
        print(f"ğŸ§  Active Inference Agent Initialized.")
        print(f"   Actions: {action_size}, Curiosity: {epsilon:.2f}")
        print(f"   Kaname Integration: {'âœ…' if self.brain else 'âŒ'}")
    
    def _init_kaname_systems(self):
        """Kaname ã‚·ã‚¹ãƒ†ãƒ ã¸ã®å‚ç…§ã‚’åˆæœŸåŒ–"""
        if not self.brain:
            return
        
        # MetaLearner
        if hasattr(self.brain, 'meta_learner'):
            self.meta_learner = self.brain.meta_learner
            print("   ğŸ“Š MetaLearner connected")
        
        # WorldModel
        if hasattr(self.brain, 'world_model'):
            self.world_model = self.brain.world_model
            print("   ğŸŒ WorldModel connected")
        
        # GeologicalMemory
        if hasattr(self.brain, 'cortex') and self.brain.cortex:
            if hasattr(self.brain.cortex, 'memory'):
                self.memory = self.brain.cortex.memory
                print("   ğŸª¨ GeologicalMemory connected")
    
    def _state_to_key(self, state: np.ndarray) -> str:
        """çŠ¶æ…‹ã‚’ãƒãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã«å¤‰æ›ï¼ˆè¨˜æ†¶ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”¨ï¼‰"""
        if not isinstance(state, np.ndarray):
            state = np.array(state)
        
        if state.ndim == 3 and state.shape[-1] == 3: small = state
        elif state.ndim > 1: small = state[0] if state.ndim == 3 else state
        else: small = state

        small = (small // 32).flatten()
        return small[:1000].tobytes().hex()
    
    def _get_learning_rate(self) -> float:
        """MetaLearner ã‹ã‚‰å­¦ç¿’ç‡ï¼ˆé©å¿œåº¦ï¼‰ã‚’å–å¾—"""
        if self.meta_learner:
            return self.meta_learner.learning_rate
        return 0.1
    
    def select_action(self, state: np.ndarray, game_type: str = "generic") -> int:
        """
        ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠï¼ˆèƒ½å‹•çš„æ¨è«–: Active Inferenceï¼‰
        
        1. WorldModel ã§å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¾Œã®çŠ¶æ…‹ã‚’äºˆæ¸¬
        2. äºˆæ¸¬çŠ¶æ…‹ã®ã€Œæœ›ã¾ã—ã•ï¼ˆã‚´ãƒ¼ãƒ«é©åˆåº¦ï¼‰ã€ã‚’è©•ä¾¡
        3. æœ€ã‚‚æœ›ã¾ã—ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠ
        4. ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„å ´åˆã¯ã€Œå¥½å¥‡å¿ƒã€ã§æ¢ç´¢
        """
        # å¥½å¥‡å¿ƒï¼ˆæ¢ç´¢ï¼‰ãƒã‚§ãƒƒã‚¯
        exploration_rate = self.epsilon
        if self.meta_learner:
            exploration_rate = self.meta_learner.exploration_rate
        
        if random.random() < exploration_rate:
            return random.randint(0, self.action_size - 1)
        
        # WorldModel ã«ã‚ˆã‚‹äºˆæ¸¬ã¨è¨ˆç”»
        if self.world_model:
            state_vec = self._state_to_vector(state)
            
            # ç°¡æ˜“çš„ãªã‚´ãƒ¼ãƒ«: ã‚¹ã‚³ã‚¢ãŒå¢—ãˆã‚‹ã“ã¨
            # æœ¬æ¥ã¯ GeologicalMemory å†…ã®ã€Œå¿«ã€ã®æ¦‚å¿µã«è¿‘ã¥ãã“ã¨ã‚’ç›®æŒ‡ã™ã¹ã
            
            best_action = self.world_model.get_best_action(
                current_state=state_vec,
                goal_state={"score": 1.0},  # ç†æƒ³çŠ¶æ…‹
                available_actions=list(range(self.action_size))
            )
            if best_action is not None:
                return best_action
        
        # ãƒ¢ãƒ‡ãƒ«ãŒãªã„ã€ã¾ãŸã¯äºˆæ¸¬ä¸èƒ½ãªå ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ 
        return random.randint(0, self.action_size - 1)
    
    def _state_to_vector(self, state: np.ndarray) -> Dict[str, float]:
        """çŠ¶æ…‹ã‚’ãƒ™ã‚¯ãƒˆãƒ«å½¢å¼ã«å¤‰æ›"""
        if state.ndim > 1:
            flat = state.flatten()[:20] # ç‰¹å¾´é‡æŠ½å‡ºï¼ˆå°‘ã—å¢—ã‚„ã™ï¼‰
        else:
            flat = state[:20]
        
        return {f"s{i}": float(v) / 255.0 for i, v in enumerate(flat)}
    
    def remember(self, state, action, reward, next_state, done, game_type: str = "generic"):
        """
        çµŒé¨“ã‚’å‡¦ç†ï¼ˆWorldModelæ›´æ–° + GeologicalMemoryå †ç©ï¼‰
        â€» Q-Tableæ›´æ–°ã¯å»ƒæ­¢
        """
        # GeologicalMemory ã«è¨˜éŒ²ï¼ˆæ„Ÿæƒ…ä½“é¨“ã¨ã—ã¦ï¼‰
        if self.memory:
            # å ±é…¬ã‹ã‚‰æ„Ÿæƒ…ã‚’ç”Ÿæˆ
            emotion = max(-1.0, min(1.0, reward))
            
            # çµæœåˆ¤å®š
            outcome = "neutral"
            if reward > 0: outcome = "success"
            elif reward < 0 or done: outcome = "failure"
            
            # æ¦‚å¿µåˆ»å°: "game_snake_success" ãªã©
            # æ„Ÿæƒ…ãŒä¼´ã†å ´åˆã®ã¿åœ°å½¢ã‚’éš†èµ·/æ²ˆé™ã•ã›ã‚‹
            if abs(emotion) > 0.01:
                game_concept = f"gm_{game_type}_{outcome}"
                self.memory.modify_terrain(game_concept, emotion * 20)
                
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çš„ãªã‚­ãƒ¼è¨˜æ†¶ï¼ˆä½ç½®æƒ…å ±ã¨ã—ã¦ä¿å­˜ï¼‰
                # state_key = self._state_to_key(state)
                # self.memory.reinforce(state_key[:10], emotion) # ã‚ã¾ã‚Šæ„å‘³ãªã„ã‹ã‚‚ï¼Ÿ
        
        # WorldModel ã‚’æ›´æ–°ï¼ˆä¸–ç•Œã®æ³•å‰‡ã‚’å­¦ã¶ï¼‰
        if self.world_model:
            state_vec = self._state_to_vector(state)
            next_state_vec = self._state_to_vector(next_state)
            action_str = str(action)
            
            # äºˆæ¸¬èª¤å·®ã‚’å–å¾—ã—ã¦å­¦ç¿’
            error = self.world_model.update(state_vec, next_state_vec, action_str)
            self.prediction_errors.append(error)
        
        self.total_steps += 1
    
    def learn(self) -> float:
        """
        å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆMetaLearneré€£æºã®ã¿ï¼‰
        â€» Qå­¦ç¿’ã® update ã¯è¡Œã‚ãªã„
        """
        # MetaLearner ã«äºˆæ¸¬èª¤å·®ã‚’å ±å‘Šã—ã¦å­¦ç¿’ç‡ï¼ˆCuriosityï¼‰ã‚’èª¿æ•´
        if self.meta_learner and self.prediction_errors:
            avg_error = sum(self.prediction_errors[-10:]) / len(self.prediction_errors[-10:])
            self.meta_learner.adapt_learning_rate(avg_error)
        
        self.training_steps += 1
        
        # å¥½å¥‡å¿ƒã®æ¸›è¡°
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        return 0.0 # ãƒ€ãƒŸãƒ¼
    
    def record_outcome(self, success: bool):
        """çµæœã‚’MetaLearnerã«å ±å‘Š"""
        if self.meta_learner:
            self.meta_learner.record_outcome(success)
    
    def end_episode(self, final_score: float = 0.0):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†æ™‚ã®å‡¦ç†"""
        self.episode_count += 1
        
        # MetaLearner ã«çµæœã‚’å ±å‘Š
        success = final_score > 0
        self.record_outcome(success)
        
        # äºˆæ¸¬èª¤å·®å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
        self.prediction_errors = self.prediction_errors[-100:]
    
    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆã‚’å–å¾—"""
        stats = {
            "curiosity": round(self.epsilon, 4),
            "total_steps": self.total_steps,
            "training_steps": self.training_steps,
            "episode_count": self.episode_count,
            "type": "Active Inference (No RL)"
        }
        
        # Kaname ã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹ã‚’è¿½åŠ 
        if self.meta_learner:
            stats["meta_learning_rate"] = round(self.meta_learner.current_learning_rate, 4)
        if self.world_model:
            if hasattr(self.world_model, 'transition_table'):
                 stats["world_model_states"] = len(self.world_model.transition_table)
        if self.prediction_errors:
            stats["avg_prediction_error"] = round(
                sum(self.prediction_errors[-10:]) / len(self.prediction_errors[-10:]), 4
            )
        
        return stats


# ãƒ†ã‚¹ãƒˆç”¨
if __name__ == "__main__":
    print("Active Inference Agent Test")
    
    agent = IntegratedRLAgent(action_size=4)
    
    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    dummy_state = np.random.randint(0, 256, (4, 84, 84), dtype=np.uint8)
    
    for i in range(50):
        action = agent.select_action(dummy_state, "test_game")
        # next_state, reward...
        
    print(f"\nStats: {agent.get_stats()}")
    print("Done!")
