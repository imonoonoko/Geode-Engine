import time
import random
import numpy as np
import math
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
import threading

@dataclass
class FreeEnergyComponent:
    """è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®å†…è¨³"""
    action: int
    risk: float      # Divergence (Priorã¨ã®ä¹–é›¢)
    ambiguity: float # Uncertainty (äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§)
    total_ef: float  # Expected Free Energy

class ActiveInferenceAgent:
    """
    Kaname Active Inference Agent (Phase 1: Dark Room)
    
    å ±é…¬æœ€å¤§åŒ–(RL)ã§ã¯ãªãã€æœŸå¾…è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–(Active Inference)ã§å‹•ãã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
    
    G(Ï€) = Î£ P(o,s|Ï€) ln [P(o,s|Ï€) / P(o,s)]
         â‰ˆ Risk + Ambiguity
         
    - Risk (Divergence): äºˆæ¸¬ã•ã‚Œã‚‹çŠ¶æ…‹ãŒã€å¥½ã¾ã—ã„çŠ¶æ…‹(Prior)ã‹ã‚‰ã©ã‚Œã ã‘é›¢ã‚Œã¦ã„ã‚‹ã‹ã€‚
    - Ambiguity (Entropy): äºˆæ¸¬ã•ã‚Œã‚‹çŠ¶æ…‹ãŒã€ã©ã‚Œã ã‘ä¸ç¢ºå®Ÿã‹ã€‚
    """
    
    def __init__(self, 
                 action_size: int,
                 brain=None,
                 precision: float = 1.0, # è¡Œå‹•é¸æŠã®æ±ºå®šè«–çš„åº¦åˆã„
                 curiosity: float = 2.0): # å¥½å¥‡å¿ƒä¿‚æ•° (>1.0 ã§ä¸ç¢ºå®Ÿæ€§ã‚’å¥½ã‚€)
        self.action_size = action_size
        self.brain = brain
        self.precision = precision
        self.curiosity = curiosity
        self.flow_state = 0.0
        self.prediction_errors: List[float] = [] # äºˆæ¸¬èª¤å·®ã®å±¥æ­´
        
        self.lock = threading.Lock()
        
        # çµ±è¨ˆ
        self.total_steps = 0
        self.episode_count = 0
        self.last_free_energy_components: List[FreeEnergyComponent] = []
        
        # Kaname ã‚·ã‚¹ãƒ†ãƒ ã¸ã®å‚ç…§
        self.meta_learner = None
        self.world_model = None
        self.memory = None
        
        self._init_kaname_systems()
        
        print(f"ğŸ§  Active Inference Agent (Pure) Initialized.")
        print(f"   Actions: {action_size}, Precision: {precision}, Curiosity: {curiosity}")

    def _init_kaname_systems(self):
        """Kaname ã‚·ã‚¹ãƒ†ãƒ ã¸ã®å‚ç…§ã‚’åˆæœŸåŒ–"""
        if not self.brain:
            return
        
        if hasattr(self.brain, 'meta_learner'):
            self.meta_learner = self.brain.meta_learner
        
        if hasattr(self.brain, 'world_model'):
            self.world_model = self.brain.world_model
        
        if hasattr(self.brain, 'cortex') and self.brain.cortex:
            if hasattr(self.brain.cortex, 'memory'):
                self.memory = self.brain.cortex.memory

    def _state_to_vector(self, state: np.ndarray) -> np.ndarray:
        """çŠ¶æ…‹ã‚’ãƒ™ã‚¯ãƒˆãƒ«å½¢å¼(Latent)ã«å¤‰æ›"""
        # æœ¬æ¥ã¯VAEãªã©ã§åœ§ç¸®ã™ã¹ãã€‚
        # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° + Flatten
        if not isinstance(state, np.ndarray):
            state = np.array(state)
            
        if state.ndim == 3: # (C, H, W) or (H, W, C)
            # ç°¡æ˜“ç‰¹å¾´é‡: å¹³å‡è¼åº¦ã€ä¸­å¿ƒè¼åº¦ã€ã‚¨ãƒƒã‚¸é‡ãªã©ã‚’æ··ãœã‚‹
             flat = state.flatten()
             return flat[:64].astype(np.float32) / 255.0 # ä»®: æœ€åˆã®64ãƒ”ã‚¯ã‚»ãƒ«
        return np.zeros(64, dtype=np.float32)

    def _get_prior(self, current_z: np.ndarray) -> np.ndarray:
        """
        GeologicalMemory (Prior) ã‹ã‚‰ã€Œã‚ã‚‹ã¹ãçŠ¶æ…‹ã€ã‚’å–å¾—ã€‚
        
        Phase 1 (Dark Room) ã§ã¯:
        - ã¾ã è¨˜æ†¶ãŒãªã„ãŸã‚ã€ã€Œç¾åœ¨ã®çŠ¶æ…‹ã€ã¾ãŸã¯ã€Œä½•ã‚‚ãªã„çŠ¶æ…‹ã€ãŒPriorã¨ãªã‚‹ã€‚
        - ã“ã“ã§ã¯ã€Œç¾çŠ¶ç¶­æŒãƒã‚¤ã‚¢ã‚¹ã€ã‚’è¡¨ç¾ã™ã‚‹ãŸã‚ã€current_z ã‚’ãã®ã¾ã¾è¿”ã™ã€‚
          (ï¼å‹•ããŸããªã„)
        """
        # å°†æ¥çš„ã«ã¯: self.memory.get_attractor(current_z)
        return current_z

    def select_action(self, state: np.ndarray, game_type: str = "generic") -> int:
        """
        æœŸå¾…è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ (EFE) ã‚’æœ€å°åŒ–ã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠ
        
        G = Risk + Ambiguity - CuriosityBonus
        """
        current_z = self._state_to_vector(state)
        preferred_z = self._get_prior(current_z)
        
        efe_scores = []
        self.last_free_energy_components = []

        # å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æœªæ¥ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        for action in range(self.action_size):
            predicted_z = current_z # Default: ç¾çŠ¶ç¶­æŒ
            uncertainty = 1.0       # Default: éå¸¸ ä¸ç¢ºå®Ÿ
            
            if self.world_model:
                # WorldModelãŒã‚ã‚Œã°äºˆæ¸¬
                # (ActiveInferenceç”¨ã«ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹èª¿æ•´ãŒå¿…è¦ã‹ã‚‚)
                # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã€ŒWorldModelãŒäºˆæ¸¬ã—ãŸã¤ã‚‚ã‚Šã€ã®å€¤ã‚’ç”Ÿæˆ
                if action == 0: # NO_OP (Stay) assumption
                    predicted_z = current_z
                    uncertainty = 0.1 # è‡ªä¿¡ã‚ã‚Š
                else:
                    # å‹•ãã¨çŠ¶æ…‹ãŒå¤‰ã‚ã‚‹ãŒã€ã©ã†å¤‰ã‚ã‚‹ã‹ã¾ã çŸ¥ã‚‰ãªã„ -> ä¸ç¢ºå®Ÿæ€§å¤§
                    noise = np.random.normal(0, 0.1, size=current_z.shape)
                    predicted_z = current_z + noise
                    uncertainty = 2.0 # è‡ªä¿¡ãªã—
            else:
                # ãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã‚‚ã€Œä¿¡å¿µã€ã¨ã—ã¦ç‰©ç†æ³•å‰‡ã‚’æŒã¤
                if action == 0:
                    predicted_z = current_z
                    uncertainty = 0.1
                else:
                    predicted_z = current_z
                    uncertainty = 2.0
            
            # --- 1. Risk (Divergence) ---
            # Risk = || z_pred - z_pref ||^2
            risk = np.sum((predicted_z - preferred_z) ** 2)
            
            # --- 2. Ambiguity (Uncertainty & Curiosity) ---
            # Curiosity > 1.0 ã®å ´åˆã€Uncertainty ãŒé«˜ã„ã»ã© G ãŒä¸‹ãŒã‚‹
            # G = Risk + Ambiguity * (1 - Curiosity)
            
            ambiguity_term = uncertainty * (1.0 - self.curiosity)
            
            # --- Expected Free Energy ---
            G = risk + ambiguity_term
            
            efe_scores.append(G)
            self.last_free_energy_components.append(FreeEnergyComponent(action, risk, float(uncertainty), G))

        # EFEæœ€å°åŒ– = ç¢ºç‡åˆ†å¸ƒ (Softmax)
        # P(a) = softmax(-G * precision)
        
        G_array = np.array(efe_scores)
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­–
        G_array = G_array - np.min(G_array) 
        
        # ç¢ºç‡è¨ˆç®—
        probs = np.exp(-G_array * self.precision)
        
        if np.sum(probs) == 0:
            probs = np.ones(self.action_size) / self.action_size
        else:
            probs = probs / np.sum(probs)
            
        # ç¢ºç‡çš„ã«é¸æŠ
        selected_action = np.random.choice(self.action_size, p=probs)
        
        return int(selected_action)

    def remember(self, state, action, reward, next_state, done, game_type: str = "generic"):
        """
        Active Inference ã§ã¯ã€Œå ±é…¬ã«ã‚ˆã‚‹å¼·åŒ–ã€ã¯è¡Œã‚ãªã„ã€‚
        ä»£ã‚ã‚Šã«ã€Œãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°ã€ã‚’è¡Œã†ã€‚
        """
        self.total_steps += 1
        
        # 1. World Model Learning (äºˆæ¸¬èª¤å·®ã®æœ€å°åŒ–)
        if self.world_model:
            # self.world_model.update(...)
            pass
            
        # 2. Geological Memory Learning (Priorã®å½¢æˆ / ã‚¢ãƒˆãƒ©ã‚¯ã‚¿å­¦ç¿’)
        # Phase 3: The Epiphany (å¶ç„¶ã®æˆåŠŸã‚’å¿…ç„¶ã«å¤‰ãˆã‚‹)
        if self.memory:
            # æˆåŠŸä½“é¨“ï¼ˆå ±é…¬ > 0ï¼‰ã‚’ "gm_game_success" ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ã¨ã—ã¦åˆ»ã‚€
            if reward > 0:
                # æ„Ÿæƒ…ä¾¡: å ±é…¬ã‚’ãã®ã¾ã¾ã€Œå¿«ã€ã¨ã—ã¦åˆ»å°
                # ã“ã‚Œã«ã‚ˆã‚Šã€å°†æ¥ _get_prior ã§ã“ã®çŠ¶æ…‹ãŒã€Œæœ›ã¾ã—ã„ã€ã¨åˆ¤æ–­ã•ã‚Œã‚‹
                game_concept = f"gm_{game_type}_success"
                
                # åœ°å½¢ã‚’æ¿€ã—ãéš†èµ·ã•ã›ã‚‹ï¼ˆå¼·ã„ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ï¼‰
                # ActiveInference ã§ã¯ã€Œè°·ã€ã«è½ã¡ã‚ˆã†ã¨ã™ã‚‹ -> æ¦‚å¿µçš„ã«ã¯æ²ˆé™ã•ã›ã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸ã ãŒ
                # å®Ÿè£…ä¸Šã¯ modify_terrain(..., emotion_value) ã§æ­£ã®å€¤ãªã‚‰ã€Œå¼·åŒ–ã€ã¨è¦‹ãªã™
                self.memory.modify_terrain(game_concept, emotion_value=reward * 50.0)
                
                # ã•ã‚‰ã«ã€ç¾åœ¨ã®çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«è‡ªä½“ã‚’ã€Œè‰¯ã„çŠ¶æ…‹ã€ã¨ã—ã¦çŸ­æœŸè¨˜æ†¶ãƒãƒƒãƒ•ã‚¡ã«å…¥ã‚Œã‚‹ç­‰ã®
                # æ‹¡å¼µãŒæœ¬æ¥ã¯å¿…è¦ã ãŒã€ã¾ãšã¯æ¦‚å¿µãƒ¬ãƒ™ãƒ«ã§ã®çµåˆã‚’è¡Œã†ã€‚
            
            # å¤±æ•—ä½“é¨“ï¼ˆã‚²ãƒ¼ãƒ ã‚ªãƒ¼ãƒãƒ¼ã‹ã¤è² ã®å ±é…¬ï¼‰
            elif done and reward < 0:
                game_concept = f"gm_{game_type}_failure"
                # å«Œæ‚ªåˆºæ¿€ã¨ã—ã¦åˆ»ã‚€ï¼ˆè² ã®å€¤ï¼‰
                self.memory.modify_terrain(game_concept, emotion_value=reward * 50.0)

    def learn(self) -> float:
        """
        Active Inference ã§ã¯ã€Œå ±é…¬ã«ã‚ˆã‚‹å¼·åŒ–ã€ã¯è¡Œã‚ãªã„ã€‚
        ä»£ã‚ã‚Šã«ã€Œãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°ã€ã¨ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é©å¿œï¼ˆæˆé•·ï¼‰ã€ã‚’è¡Œã†ã€‚
        """
        self.total_steps += 1
        
        # 1. World Model Learning (äºˆæ¸¬èª¤å·®ã®æœ€å°åŒ–)
        if self.world_model:
            # æœ¬æ¥ã¯ã“ã“ã§ãƒ¢ãƒ‡ãƒ«æ›´æ–°
            pass
            
        # 2. Curiosity Decay & Flow State (é£½ã/æˆé•·/æ²¡é ­)
        # æ™‚é–“çµŒéã¨ã¨ã‚‚ã«å¥½å¥‡å¿ƒã¯æ¸›å°‘ã—ã€æ—¢çŸ¥ã®é ˜åŸŸ(AmbiguityãŒä½ã„å ´æ‰€)ã‚’å¥½ã‚€ã‚ˆã†ã«ãªã‚‹
        
        # Flow Calculation
        # æœ€è¿‘ã®äºˆæ¸¬èª¤å·®ãŒå°ã•ã„ï¼ˆä¸Šæ‰‹ãã„ã£ã¦ã„ã‚‹ï¼‰ã‹ã¤ CuriosityãŒé«˜ã„ï¼ˆæŒ‘æˆ¦ã—ã¦ã„ã‚‹ï¼‰
        # -> ã‚¾ãƒ¼ãƒ³ã«å…¥ã‚‹ (Flow State)
        
        avg_error = 1.0 # Default
        if self.prediction_errors:
             avg_error = sum(self.prediction_errors[-10:]) / len(self.prediction_errors[-10:])
        
        # èª¤å·®ãŒå°ã•ã„ = ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã§ãã¦ã„ã‚‹
        # CuriosityãŒé«˜ã„ = é€€å±ˆã—ã¦ã„ãªã„ï¼ˆæœªçŸ¥ã¸ã®æŒ‘æˆ¦ä¸­ï¼‰
        if avg_error < 0.1 and self.curiosity > 0.8:
            # Flow è“„ç©
            self.flow_state = min(1.0, self.flow_state + 0.05)
        else:
            # Flow æ¸›è¡°
            self.flow_state = max(0.0, self.flow_state - 0.01)
            
        # Flow ã«ã‚ˆã‚‹ãƒ–ãƒ¼ã‚¹ãƒˆåŠ¹æœ
        # ãƒ•ãƒ­ãƒ¼ä¸­ã¯ã€Œã‚‚ã£ã¨çŸ¥ã‚ŠãŸã„ï¼ˆCuriosityï¼‰ã€ã¨ã€Œç¢ºä¿¡ï¼ˆPrecisionï¼‰ã€ãŒåŒæ™‚ã«é«˜ã¾ã‚‹
        current_precision = self.precision * (1.0 + self.flow_state * 2.0)
        
        # Curiosity Decay (åŸºæœ¬ã¯æ¸›è¡°ã™ã‚‹ãŒã€Flowä¸­ã¯ç¶­æŒã•ã‚Œã‚‹)
        decay = 0.9995
        min_curiosity = 0.8
        
        if self.flow_state > 0.5:
             # ãƒ•ãƒ­ãƒ¼ä¸­ã¯å¥½å¥‡å¿ƒãŒæ¸›ã‚‰ãªã„ï¼ˆã‚€ã—ã‚ç¶­æŒã•ã‚Œã‚‹ï¼‰
             pass
        elif self.curiosity > min_curiosity:
            self.curiosity *= decay
            
        return 0.0

    def end_episode(self, final_score: float = 0.0):
        self.episode_count += 1
        
    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±"""
        last_action_stats = "None"
        if self.last_free_energy_components:
            # æœ€å°EFEã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æƒ…å ±ã‚’è¡¨ç¤º
            best = min(self.last_free_energy_components, key=lambda x: x.total_ef)
            last_action_stats = f"Act:{best.action} G:{best.total_ef:.2f} (R:{best.risk:.2f} A:{best.ambiguity:.2f})"
            
        return {
            "type": "Active Inference (Pure)",
            "precision": round(self.precision, 2),
            "curiosity": round(self.curiosity, 4),
            "flow": round(self.flow_state, 2),
            "last_decision": last_action_stats
        }


if __name__ == "__main__":
    # Test
    agent = ActiveInferenceAgent(action_size=3)
    dummy_state = np.zeros((3, 64, 64))
    act = agent.select_action(dummy_state)
    print(f"Selected Action: {act}")
    print(f"Stats: {agent.get_stats()}")

