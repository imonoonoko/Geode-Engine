# rl_agent.py
# Game AI Phase A-3: å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
# DQNï¼ˆDeep Q-Networkï¼‰ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

import time
import random
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
from collections import deque
import threading


@dataclass
class Experience:
    """çµŒé¨“ï¼ˆé·ç§»ï¼‰"""
    state: np.ndarray
    action: int
    reward: float
    next_state: np.ndarray
    done: bool


class ReplayBuffer:
    """
    çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡
    
    éå»ã®çµŒé¨“ã‚’ä¿å­˜ã—ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§å­¦ç¿’ã‚’å®‰å®šåŒ–
    """
    
    def __init__(self, capacity: int = 10000):
        self.buffer = deque(maxlen=capacity)
        self.lock = threading.Lock()
    
    def push(self, exp: Experience):
        """çµŒé¨“ã‚’è¿½åŠ """
        with self.lock:
            self.buffer.append(exp)
    
    def sample(self, batch_size: int) -> List[Experience]:
        """ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        with self.lock:
            return random.sample(list(self.buffer), min(batch_size, len(self.buffer)))
    
    def __len__(self):
        return len(self.buffer)


class SimpleQNetwork:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªQé–¢æ•°ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãªã—ç‰ˆï¼‰
    
    çŠ¶æ…‹ã‚’ãƒãƒƒã‚·ãƒ¥åŒ–ã—ã¦ãƒ†ãƒ¼ãƒ–ãƒ«ã§ç®¡ç†ã€‚
    å°è¦æ¨¡ãªã‚²ãƒ¼ãƒ ã‚„ã€PyTorchãªã—ã§ã®å‹•ä½œç¢ºèªç”¨ã€‚
    """
    
    def __init__(self, action_size: int, learning_rate: float = 0.1):
        self.action_size = action_size
        self.lr = learning_rate
        self.q_table: Dict[str, np.ndarray] = {}
        self.lock = threading.Lock()
    
    def _state_to_key(self, state: np.ndarray) -> str:
        """çŠ¶æ…‹ã‚’ãƒãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã«å¤‰æ›"""
        # çŠ¶æ…‹ã‚’ç²—ãé‡å­åŒ–ã—ã¦ã‚­ãƒ¼ã«ã™ã‚‹
        if state.ndim > 1:
            small = state[0] if state.ndim == 3 else state
            small = (small // 32).flatten()[:100]  # æœ€åˆã®100è¦ç´ ã®ã¿ä½¿ç”¨
        else:
            small = (state // 32)[:100]
        return small.tobytes()
    
    def get_q_values(self, state: np.ndarray) -> np.ndarray:
        """Qå€¤ã‚’å–å¾—"""
        key = self._state_to_key(state)
        with self.lock:
            if key not in self.q_table:
                self.q_table[key] = np.zeros(self.action_size)
            return self.q_table[key].copy()
    
    def update(self, state: np.ndarray, action: int, target: float):
        """Qå€¤ã‚’æ›´æ–°"""
        key = self._state_to_key(state)
        with self.lock:
            if key not in self.q_table:
                self.q_table[key] = np.zeros(self.action_size)
            # TDå­¦ç¿’
            self.q_table[key][action] += self.lr * (target - self.q_table[key][action])
    
    def get_state(self) -> Dict:
        """çŠ¶æ…‹ã‚’å–å¾—"""
        return {
            "table_size": len(self.q_table),
            "learning_rate": self.lr
        }


class RLAgent:
    """
    å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    
    Îµ-greedyæ–¹ç­–ã§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã€
    Qå­¦ç¿’ã§ä¾¡å€¤é–¢æ•°ã‚’æ›´æ–°ã™ã‚‹ã€‚
    """
    
    def __init__(self, 
                 action_size: int,
                 epsilon: float = 1.0,
                 epsilon_min: float = 0.1,
                 epsilon_decay: float = 0.995,
                 gamma: float = 0.99,
                 learning_rate: float = 0.1,
                 batch_size: int = 32):
        """
        Args:
            action_size: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°
            epsilon: æ¢ç´¢ç‡ï¼ˆåˆæœŸå€¤ï¼‰
            epsilon_min: æ¢ç´¢ç‡ï¼ˆæœ€å°å€¤ï¼‰
            epsilon_decay: æ¢ç´¢ç‡ã®æ¸›è¡°ç‡
            gamma: å‰²å¼•ç‡
            learning_rate: å­¦ç¿’ç‡
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        """
        self.action_size = action_size
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.gamma = gamma
        self.batch_size = batch_size
        
        # Qé–¢æ•°
        self.q_network = SimpleQNetwork(action_size, learning_rate)
        
        # çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤
        self.memory = ReplayBuffer(capacity=10000)
        
        # çµ±è¨ˆ
        self.total_steps = 0
        self.training_steps = 0
        self.episode_count = 0
        
        print(f"ğŸ¤– RL Agent Initialized.")
        print(f"   Actions: {action_size}, Îµ: {epsilon:.2f}")
    
    def select_action(self, state: np.ndarray) -> int:
        """
        ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠï¼ˆÎµ-greedyï¼‰
        """
        if random.random() < self.epsilon:
            # æ¢ç´¢: ãƒ©ãƒ³ãƒ€ãƒ 
            return random.randint(0, self.action_size - 1)
        else:
            # æ´»ç”¨: Qå€¤æœ€å¤§
            q_values = self.q_network.get_q_values(state)
            return int(np.argmax(q_values))
    
    def remember(self, state, action, reward, next_state, done):
        """çµŒé¨“ã‚’è¨˜æ†¶"""
        exp = Experience(state, action, reward, next_state, done)
        self.memory.push(exp)
        self.total_steps += 1
    
    def learn(self) -> float:
        """
        çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤ã‹ã‚‰å­¦ç¿’
        
        Returns:
            å¹³å‡æå¤±ï¼ˆã¾ãŸã¯0ï¼‰
        """
        if len(self.memory) < self.batch_size:
            return 0.0
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch = self.memory.sample(self.batch_size)
        
        total_loss = 0.0
        for exp in batch:
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨ˆç®—
            if exp.done:
                target = exp.reward
            else:
                next_q = self.q_network.get_q_values(exp.next_state)
                target = exp.reward + self.gamma * np.max(next_q)
            
            # æ›´æ–°
            current_q = self.q_network.get_q_values(exp.state)[exp.action]
            loss = abs(target - current_q)
            total_loss += loss
            
            self.q_network.update(exp.state, exp.action, target)
        
        self.training_steps += 1
        
        # Îµæ¸›è¡°
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        return total_loss / len(batch)
    
    def end_episode(self):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†æ™‚ã®å‡¦ç†"""
        self.episode_count += 1
    
    def get_stats(self) -> Dict:
        """çµ±è¨ˆã‚’å–å¾—"""
        return {
            "epsilon": round(self.epsilon, 4),
            "total_steps": self.total_steps,
            "training_steps": self.training_steps,
            "episode_count": self.episode_count,
            "memory_size": len(self.memory),
            "q_table_size": self.q_network.get_state()["table_size"]
        }


# ãƒ†ã‚¹ãƒˆç”¨
if __name__ == "__main__":
    print("RL Agent Test")
    
    agent = RLAgent(action_size=4)
    
    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    dummy_state = np.random.randint(0, 256, (4, 84, 84), dtype=np.uint8)
    
    for i in range(100):
        action = agent.select_action(dummy_state)
        next_state = np.random.randint(0, 256, (4, 84, 84), dtype=np.uint8)
        reward = random.random() - 0.5
        done = random.random() < 0.05
        
        agent.remember(dummy_state, action, reward, next_state, done)
        agent.learn()
        
        dummy_state = next_state
        
        if done:
            agent.end_episode()
    
    print(f"\nStats: {agent.get_stats()}")
    print("Done!")
