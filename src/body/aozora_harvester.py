# aozora_harvester.py
# é’ç©ºæ–‡åº«ã‹ã‚‰è‡ªå‹•çš„ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åç©«ã™ã‚‹

import requests
import random
import re
import os
import time
from typing import Optional

class AozoraHarvester:
    """
    é’ç©ºæ–‡åº«ã‹ã‚‰è‡ªå‹•çš„ã«ä½œå“ã‚’å–å¾—
    é€€å±ˆæ™‚ã« Kaname ãŒè‡ªåˆ†ã§é£Ÿæ–™ã‚’æ¢ã—ã«è¡Œã
    
    50+ ä½œå“ã‹ã‚‰é¸æŠå¯èƒ½
    """
    
    # ä½œå“ãƒªã‚¹ãƒˆ (AuthorID, WorkID, Title)
    # ã“ã‚Œã«ã‚ˆã‚Šã‚«ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸(card{WorkID}.html)ã‹ã‚‰æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«URLã‚’å–å¾—ã™ã‚‹
    WORKS = [
        # å®®æ²¢è³¢æ²» (81)
        (81, 456, "éŠ€æ²³é‰„é“ã®å¤œ"),
        (81, 43754, "æ³¨æ–‡ã®å¤šã„æ–™ç†åº—"),
        (81, 470, "ã‚»ãƒ­å¼¾ãã®ã‚´ãƒ¼ã‚·ãƒ¥"),
        (81, 462, "é¢¨ã®åˆä¸‰éƒ"),
        (81, 46605, "ã‚„ã¾ãªã—"),
        (81, 45630, "é›¨ãƒ‹ãƒ¢ãƒã‚±ã‚º"),
        # å¤ªå®°æ²» (35)
        (35, 1567, "èµ°ã‚Œãƒ¡ãƒ­ã‚¹"),
        (35, 301, "äººé–“å¤±æ ¼"),
        (35, 1565, "æ–œé™½"),
        (35, 2253, "ãƒ´ã‚£ãƒ¨ãƒ³ã®å¦»"),
        (35, 307, "ãŠä¼½è‰ç´™"),
        (35, 275, "å¥³ç”Ÿå¾’"),
        # èŠ¥å·é¾ä¹‹ä»‹ (879)
        (879, 127, "ç¾…ç”Ÿé–€"),
        (879, 92, "èœ˜è››ã®ç³¸"),
        (879, 42, "é¼»"),
        (879, 179, "è—ªã®ä¸­"),
        (879, 60, "åœ°ç„å¤‰"),
        (879, 43016, "ãƒˆãƒ­ãƒƒã‚³"),
        # å¤ç›®æ¼±çŸ³ (148)
        (148, 789, "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹"),
        (148, 773, "ã“ã“ã‚"),
        (148, 752, "åŠã¤ã¡ã‚„ã‚“"),
        (148, 799, "å¤¢åå¤œ"),
        (148, 776, "è‰æ•"),
        # å¯ºç”°å¯…å½¦ (42)
        (42, 2362, "èŒ¶ç¢—ã®æ¹¯"),
        (42, 1684, "æŸ¿ã®ç¨®"),
        # ä¸­å³¶æ•¦ (119)
        (119, 624, "å±±æœˆè¨˜"),
        (119, 1737, "æé™µ"),
        (119, 621, "å¼Ÿå­"),
        # æ¢¶äº•åŸºæ¬¡éƒ (74)
        (74, 424, "æª¸æª¬"),
        (74, 427, "æ¡œã®æ¨¹ã®ä¸‹ã«ã¯"),
        # æ–°ç¾å—å‰ (121)
        (121, 628, "ã”ã‚“ç‹"),
        (121, 637, "æ‰‹è¢‹ã‚’è²·ã„ã«"),
        # å‚å£å®‰å¾ (1095)
        (1095, 42620, "å •è½è«–"),
        (1095, 42618, "æ¡œã®æ£®ã®æº€é–‹ã®ä¸‹"),
        # æ£®é·—å¤– (129)
        (129, 2078, "èˆå§«"),
        (129, 691, "é«˜ç€¬èˆŸ"),
    ]
    
    BASE_URL = "https://www.aozora.gr.jp"
    
    def __init__(self, brain_ref=None, cache_dir: str = "food"):
        self.brain_ref = brain_ref
        self.cache_dir = cache_dir
        self.harvested_count = 0
        self.last_harvest = 0.0
        self.cooldown = 300.0
        self.harvested_ids = set()  # (AuthorID, WorkID) ã§ç®¡ç†
        
        os.makedirs(cache_dir, exist_ok=True)
        print("ğŸŒ¾ Aozora Harvester Initialized.")

    def _get_random_work(self) -> Optional[tuple]:
        """ãƒ©ãƒ³ãƒ€ãƒ ãªä½œå“ã‚’å–å¾—"""
        available = [w for w in self.WORKS if (w[0], w[1]) not in self.harvested_ids]
        if not available:
            available = self.WORKS
            self.harvested_ids.clear()
        return random.choice(available)
    
    def _resolve_file_url(self, author_id: int, work_id: int) -> Optional[str]:
        """ã‚«ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ã‹ã‚‰HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®URLã‚’è§£æ±ºã™ã‚‹"""
        card_url = f"{self.BASE_URL}/cards/{author_id:06d}/card{work_id}.html"
        try:
            response = requests.get(card_url, timeout=10)
            response.encoding = 'utf-8' # ã‚«ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ã¯UTF-8ã¾ãŸã¯Shift_JISã ãŒrequestsãŒåˆ¤å®šã—ã¦ãã‚Œã‚‹ã¯ãšã€‚æ˜ç¤ºã™ã‚‹ãªã‚‰textã‚¢ã‚¯ã‚»ã‚¹å‰ã«ã€‚
            
            # HTMLãƒªãƒ³ã‚¯ã‚’æ¢ã™
            # ä¾‹: <a href="./files/456_15050.html">ã„ã¾ã™ãXHTMLç‰ˆã§èª­ã‚€</a>
            matches = re.findall(r'<a href=\"([^\"]+)\"[^>]*>ã„ã¾ã™ã[X]?HTMLç‰ˆã§èª­ã‚€</a>', response.text)
            if matches:
                # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
                rel_path = matches[0]
                # ./files/... -> files/...
                if rel_path.startswith('./'):
                    rel_path = rel_path[2:]
                return f"{self.BASE_URL}/cards/{author_id:06d}/{rel_path}"
                
            return None
        except Exception as e:
            print(f"âš ï¸ Resolve URL Error: {e}")
            return None
    
    def _download_text(self, url: str) -> Optional[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            response = requests.get(url, timeout=15)
            # é’ç©ºæ–‡åº«ã¯åŸºæœ¬çš„ã«Shift_JIS
            response.encoding = 'shift_jis'
            text = response.text
            
            # æœ¬æ–‡æŠ½å‡º: main_text div ã‚’æ¢ã™ (æ­£è¦è¡¨ç¾ã§å±æ€§ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã«å¯¾å¿œ)
            match = re.search(r'<div[^>]*class=["\']?main_text["\']?[^>]*>', text)
            
            if match:
                start = match.end()
                # çµ‚äº†ä½ç½®ã‚’æ¢ã™: æ›¸èªŒæƒ…å ±ã®é–‹å§‹ ã¾ãŸã¯ bodyã®çµ‚äº†
                # 1. æ›¸èªŒæƒ…å ±ã®å‰ã¾ã§
                end_match = re.search(r'<div[^>]*class=["\']?bibliographical_information["\']?[^>]*>', text[start:])
                if end_match:
                    end = start + end_match.start()
                else:
                    # 2. æ›¸èªŒæƒ…å ±ãŒãªã„å ´åˆã€ãƒ¡ã‚¤ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®çµ‚äº†ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ¢ã™
                    end_comment = text.find('</div><!--/main_text-->', start)
                    if end_comment != -1:
                        end = end_comment
                    else:
                        # 3. bodyã®çµ‚äº†ã¾ã§
                        body_end = text.rfind('</body>')
                        if body_end != -1:
                            end = body_end
                        else:
                            end = len(text)
                            
                main_text = text[start:end]
            
            else:
                # main_text ãŒãªã„å ´åˆ (å¤ã„å½¢å¼ãªã©)ã€bodyå…¨ä½“ã‹ã‚‰æŠ½å‡ºã‚’è©¦ã¿ã‚‹
                body_start = re.search(r'<body[^>]*>', text)
                if body_start:
                    start = body_start.end()
                    body_end = text.rfind('</body>')
                    end = body_end if body_end != -1 else len(text)
                    main_text = text[start:end]
                else:
                    return None
            
            if not main_text or len(main_text) < 100:
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºã™
                print(f"âš ï¸ Content too short ({len(main_text) if 'main_text' in locals() else 0} chars)")
                return None
            
            # HTMLã‚¿ã‚°ã‚’é™¤å»
            main_text = re.sub(r'<[^>]+>', '', main_text)
            # ãƒ«ãƒ“æ³¨è¨˜ã‚’é™¤å»
            main_text = re.sub(r'ã€Š[^ã€‹]+ã€‹', '', main_text)
            main_text = re.sub(r'ï¼»[^ï¼½]+ï¼½', '', main_text)
            main_text = re.sub(r'ï½œ', '', main_text)
            # é€£ç¶šç©ºç™½ã‚’æ•´ç†
            main_text = re.sub(r'\s+', ' ', main_text)
            
            return main_text.strip()
            
        except Exception as e:
            print(f"âš ï¸ Aozora Download Error: {e}")
            return None
    
    def harvest(self) -> Optional[str]:
        """
        é’ç©ºæ–‡åº«ã‹ã‚‰1ä½œå“ã‚’åç©«
        
        Returns: ä½œå“ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæˆåŠŸæ™‚ï¼‰ã€Noneï¼ˆå¤±æ•—æ™‚ï¼‰
        """
        # ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ãƒã‚§ãƒƒã‚¯
        if time.time() - self.last_harvest < self.cooldown:
            return None
        
        print("ğŸŒ¾ [Aozora] Harvesting...")
        
        work = self._get_random_work()
        if not work:
            print("âš ï¸ [Aozora] No work found.")
            return None
        
        author_id, work_id, title = work
        print(f"ğŸ“š ä½œå“ID:{work_id}ã€{title}ã€ã®URLã‚’è§£æ±ºä¸­...")
        
        # ã‚«ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«URLã‚’è§£æ±º
        url = self._resolve_file_url(author_id, work_id)
        if not url:
            print(f"âš ï¸ [Aozora] Failed to resolve URL for {title}")
            return None
            
        text = self._download_text(url)
        if not text or len(text) < 100:
            print("âš ï¸ [Aozora] Text too short or empty.")
            return None
        
        self.harvested_count += 1
        self.last_harvest = time.time()
        self.harvested_ids.add((author_id, work_id))
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        # IDãƒ™ãƒ¼ã‚¹ã®åå‰ã«å¤‰æ›´ã—ã¦è¡çªå›é¿
        filename = f"aozora_{author_id}_{work_id}_{int(time.time())}.txt"
        filepath = os.path.join(self.cache_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(text)
        
        print(f"ğŸŒ¾ [Aozora] Harvested: ã€{title}ã€({len(text):,} chars)")
        
        return text
    
    def hungry_harvest(self) -> bool:
        """
        é€€å±ˆæ™‚ã«è‡ªå‹•ã§åç©«
        brain_ref ãŒå¿…è¦
        
        Returns: True if harvested
        """
        if not self.brain_ref:
            return False
        
        try:
            from src.body.hormones import Hormone
            boredom = self.brain_ref.hormones.get(Hormone.BOREDOM)
            stimulation = self.brain_ref.hormones.get(Hormone.STIMULATION)
            
            # é€€å±ˆã‹ã¤åˆºæ¿€ä¸è¶³ â†’ åç©«ã«å‡ºã‚‹
            if boredom > 60 and stimulation < 30:
                text = self.harvest()
                if text:
                    # ç›´æ¥èƒƒè¢‹ã«é€ã‚Šè¾¼ã‚€
                    if hasattr(self.brain_ref, 'cortex') and hasattr(self.brain_ref.cortex, 'stomach'):
                        self.brain_ref.cortex.stomach.eat(text)
                    
                    # ãƒ›ãƒ«ãƒ¢ãƒ³æ›´æ–°
                    self.brain_ref.hormones.update(Hormone.BOREDOM, -20.0)
                    self.brain_ref.hormones.update(Hormone.STIMULATION, 30.0)
                    self.brain_ref.hormones.update(Hormone.DOPAMINE, 10.0)
                    
                    print("ğŸŒ¾ [Aozora] Fed to stomach!")
                    return True
        except Exception as e:
            print(f"âš ï¸ hungry_harvest error: {e}")
        
        return False
